{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Project: Data Wrangling and Analysis of WeRateDogs Twitter Archive.\n",
    "\n",
    "## Table of Contents\n",
    "* [Data Gathering](#data-gathering)\n",
    "* [Data Assessing](#assessing-data)\n",
    "* [Data Cleaning](#cleaning-data)\n",
    "* [Data Visualizing](#analyzing-and-visualizing-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #for visualization\n"
     ]
    }
   ],
   "source": [
    "# importing packages for this project\n",
    "import pandas as pd #for data wrangling\n",
    "import numpy as np # for mathematical computing\n",
    "import requests # for downloading files programmatically\n",
    "import os # for accessing downloaded files\n",
    "import tweepy # to query twitter API\n",
    "import json # to write a json data off the querried data#\n",
    "import time # time module allows to work with time#\n",
    "import matplotlib.pyplot as plt #for visualization\n",
    "%matplotlib inline #for visualization\n",
    "import seaborn as sns #for visualization\n",
    "import sklearn\n",
    "import datetime\n",
    "import io"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "In the cell below, gather **all** three pieces of data for this project and load them in the notebook. **Note:** the methods required to gather each data are different.\n",
    "1. Directly download the WeRateDogs Twitter archive data (twitter_archive_enhanced.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# reading the downloaded file to pandas \n",
    "twitter_archive = pd.read_csv('../data/raw_data/twitter-archive-enhanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view read file\n",
    "twitter_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the Requests library to download the tweet image prediction (image_predictions.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a request for image_predictions file\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "\n",
    "# accessing the content of downloaded file and writing to a file\n",
    "with open(os.path.join('../data/raw_data/image-predictions.tsv'), mode = 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# reading image predictions file to pandas\n",
    "image_predictions = pd.read_csv('../data/raw_data/image-predictions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view image_predictions file\n",
    "image_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the Tweepy library to query additional data via the Twitter API (tweet_json.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "# These are hidden to comply with Twitter's API terms and conditions\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "access_secret = 'HIDDEN'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE TO REVIEWER: this student had mobile verification issues so the following\n",
    "# Twitter API code was sent to this student from a Udacity instructor\n",
    "# Tweet IDs for which to gather additional data via Twitter's API\n",
    "tweet_ids = twitter_archive.tweet_id.values\n",
    "len(tweet_ids)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            # Getting the status data for each tweet ID\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"success\")\n",
    "            # to convert each tweet status to JSON string and save\n",
    "            json.dump(tweet._json, outfile)\n",
    "            # break text with \\n\n",
    "            outfile.write('\\n')\n",
    "            \n",
    "        # to contain potential errors from above loop\n",
    "        except tweepy.TweepyException as e:\n",
    "            print(\"failed\")\n",
    "            #writes error to the fails_dict dictionary\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract the 'id', 'retweet_count', 'favorite_count', 'followers_count',\n",
    "# 'friends_count', 'listed_count' from 'tweet_json'\n",
    "# and later convert to a DataFrame\n",
    "\n",
    "#create and empty list to house the extracted data\n",
    "df_list =[]\n",
    "\n",
    "# open .txt file for reading.\n",
    "with open ('../data/tweet_json.txt', 'r') as jsonfile:\n",
    "    for line in jsonfile.readlines():\n",
    "         # read json string into a dictionary\n",
    "        tweet_line = json.loads(line)\n",
    "        # getting the required parameters\n",
    "        tweet_ID = tweet_line['id']\n",
    "        retweet_count = tweet_line['retweet_count']\n",
    "        friends_count = tweet_line['user']['friends_count']\n",
    "        fav_count = tweet_line['favorite_count']\n",
    "        followers_count = tweet_line['user']['followers_count']\n",
    "        listed_count = tweet_line['user']['listed_count']\n",
    "        \n",
    "        \n",
    "        # Append to list of dictionaries\n",
    "        df_list.append({'id': tweet_ID,\n",
    "                       'retweet_count': retweet_count,\n",
    "                       'friends_count': friends_count,\n",
    "                       'favorite_count': fav_count,\n",
    "                       'followers_count': followers_count,\n",
    "                       'listed_count': listed_count})\n",
    "        \n",
    "# creating a dataframe off the dictionaries\n",
    "tweet_json = pd.DataFrame(df_list, columns=['id', 'retweet_count',\n",
    "                                            'friends_count',\n",
    "                                            'favorite_count', 'followers_count', \n",
    "                                            'listed_count'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 28,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Assessing Data\n",
    "In this section, detect and document at least **eight (8) quality issues and two (2) tidiness issue**. You must use **both** visual assessment\n",
    "programmatic assessement to assess the data.\n",
    "\n",
    "**Note:** pay attention to the following key points when you access the data.\n",
    "\n",
    "* You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "* Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\n",
    "* The fact that the rating numerators are greater than the denominators does not need to be cleaned. This [unique rating system](http://knowyourmeme.com/memes/theyre-good-dogs-brent) is a big part of the popularity of WeRateDogs.\n",
    "* You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easy assess, rename the dataframes\n",
    "df1 = twitter_archive\n",
    "df2 = image_predictions\n",
    "df3 = tweet_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Assessment:\n",
    "Here, a directed visual assessment of the dataframes will be carried out, aiming to explain the columns and check for anomalous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`df1`** columns: \n",
    "\n",
    "1. **`tweet_id`**: this's the unique tweet identifier\n",
    "2. **`in_reply_to_status_id`**: contains the integer representation of the original tweet's ID, if the `tweet_id` is a reply.\n",
    "3. **`in_reply_to_user_id`**: if `tweet_id` is a reply, this contains the integer representation of the original Tweet's author ID.\n",
    "4. **`timestamp`**: contains time when the tweet was created.\n",
    "5. **`source`**: contains a display of the devices through which the tweet was created.\n",
    "6. **`text`**: the text element of the tweet\n",
    "7. **`retweeted_status_id`**: contains interger representation of the original `tweet_id` if `tweet_id` is a retweet.\n",
    "8. **`retweeted_status_user_id`**: if `tweet_id` is a retweet, this displays the integer representaion of the original Tweet's author ID.\n",
    "9. **`retweeted_status_timestamp`**: time of retweet.\n",
    "10. **`expanded_urls`**: tweet's URL.\n",
    "11. **`rating_numerator`**: conatains the numerator of the rating of a dog. ratings are almost always greater than 10.\n",
    "12. **`name`**: name of the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`df2`** columns:\n",
    "\n",
    "1. **`tweet_id`**: the unique identifier for each tweet\n",
    "2. **`jpg_url`**: dog's image URL\n",
    "3. **`img_num`**: the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "4. **`p1`**: algorithm's #1 prediction for the image in the tweet\n",
    "5. **`p1_conf`**: how confident the algorithm is in its #1 prediction.\n",
    "6. **`p1_dog`**: whether or not the #1 prediction is a breed of dog\n",
    "7. **`p2`**: algorithm's #2 prediction for the image in the tweet\n",
    "8. **`p2_conf`**: how confident the algorithm is in its #2 prediction.\n",
    "9. **`p2_dog`**: whether or not the #2 prediction is a breed of dog.\n",
    "10. **`p3`**: algorithm's #3 prediction for the image in the tweet.\n",
    "11. **`p3_conf`**: how confident the algorithm is in its #3 prediction.\n",
    "12. **`p3_dog`**: whether or not the #3 prediction is a breed of dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`df3`** columns:\n",
    "\n",
    "1. **`id`**: the unique identifier for each tweet.\n",
    "2. **`retweet_count`**: the number of times the original tweet was retweeted.\n",
    "3. **`favorite_count`**: the number of times the the original tweet was loved or liked.\n",
    "4. **`followers_count`**: the number of followers of WeRataeDogs account as at the time of the each tweet.\n",
    "5. **`friends_count`**: the number of profiles WeRateDogs account was following at the time of each tweet.\n",
    "6. **`listed_count`**: The number of public lists that this user is a member of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic Assessment\n",
    "Here, directed assessment using different pandas function will be used to assess the three(3) dataframe,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the `.info()` function to get a summary of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the statistical description of the DataFrames:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for `null` values in the DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df3.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for duplicated values in the DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.name.value_counts().head(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality issues\n",
    "#### Twitter_archive (df1)\n",
    "1. **`Tweet_id`** should be a string not integer\n",
    "2. **`Timestamp`** is Datetime not object\n",
    "3. **`Tweet_id`** with **`Retweeted_status_id`**, **`Retweeted_status_user_id`**, **`Retweeted_status_timestamp`** should be dropped.\n",
    "4. **`Retweeted_status_timestamp`** is datetime not object.\n",
    "5. Missing values in **`Retweeted_status_id`**\n",
    "6. Missing values in **`Retweeted_status_user_id`**\n",
    "7. Missing values in **`Retweeted_status_timestamp`**\n",
    "8. Missing values in **`Expanded_urls`**\n",
    "9. Missing values in **`in_Reply_To_Status_Id`**\n",
    "10. Missing values in **`In_Reply_To_User_id`**\n",
    "11. **`name`** value contains invalid names in lower case.\n",
    "12. Take out url at the end of some strings in `text column`\n",
    "\n",
    "#### Image_Predictions (df2)\n",
    "13. **`Tweet_id`** has wrong datatype.\n",
    "\n",
    "#### Tweet_json\n",
    "14. **`id`** has wrong datatype.\n",
    "15. wrong column name **`id`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Tidiness issues\n",
    "#### Twitter_archive (df1)\n",
    "15. `friends_count` column has only one value (104).\n",
    "16. Rating should be in a single column\n",
    "17. **`doggo`**, **`floofer`**, **`pupper`** and **`puppo`** columns need to be one column **`dog_stage`** with 4 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merging tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "<a id='Data Cleaning'></a>\n",
    "## Cleaning Data\n",
    "In this section, clean **all** of the issues you documented while assessing. \n",
    "\n",
    "**Note:** Make a copy of the original data before cleaning. Cleaning includes merging individual pieces of data according to the rules of [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). The result should be a high-quality and tidy master pandas DataFrame (or DataFrames, if appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of original pieces of data\n",
    "df1_clean = df1.copy()\n",
    "df2_clean = df2.copy()\n",
    "df3_clean = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #1: datatype issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:\n",
    "* In df1 change **timestamp** datatype from object to datetime.\n",
    "* In df1 change **tweet_id** datatype  from int to string.\n",
    "* In df1 change **Retweeted_status_timestamp** datatype from object to datetime.\n",
    "* In df2 change **tweet_id** from int to string.\n",
    "* In df3 change **id** datatype from int to string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the datetime class of the datetime module for this conversion\n",
    "df1_clean['timestamp'] = pd.to_datetime(df1_clean['timestamp'])\n",
    "\n",
    "# change tweet_id column from int to string\n",
    "df1_clean['tweet_id'] = df1_clean['tweet_id'].astype(str)\n",
    "\n",
    "# change Retweeted_status_timestamp datatype from object to datetime.\n",
    "df1_clean['retweeted_status_timestamp'] = pd.to_datetime(df1_clean['retweeted_status_timestamp'])\n",
    "\n",
    "# In df2 change **tweet_id** from int to string.\n",
    "df2_clean['tweet_id'] = df2_clean['tweet_id'].astype(str)\n",
    "\n",
    "# In df3 change **id** datatype from int to string.\n",
    "df3_clean['id'] = df3_clean['id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dtype method to check df1_clean\n",
    "df1_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dtype method to check df2_clean\n",
    "df2_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dtype method to check df3\n",
    "df3_clean.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #2: Erroneous IDs in df1 tweet_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Define\n",
    "remove rows with values for 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp' in Df1. These IDs are that of retweets and won't be used for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop retweeted rows\n",
    "df1_clean = df1_clean[df1_clean.retweeted_status_id.isnull()]\n",
    "df1_clean = df1_clean[df1_clean.retweeted_status_user_id.isnull()]\n",
    "df1_clean = df1_clean[df1_clean.retweeted_status_timestamp.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 3: Missing values\n",
    "\n",
    "* Missing values in Retweeted_status_id\n",
    "* Missing values in Retweeted_status_user_id\n",
    "* Missing values in Retweeted_status_timestamp\n",
    "* Missing values in Expanded_urls\n",
    "* Missing values in in_Reply_To_Status_Id\n",
    "* Missing values in In_Reply_To_User_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:\n",
    "* drop the above columns from df1_clean as they aren't needed in EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns\n",
    "df1_clean.drop(columns=['in_reply_to_status_id', 'in_reply_to_user_id',\n",
    "                        'retweeted_status_id', 'retweeted_status_user_id',\n",
    "                        'retweeted_status_timestamp', 'expanded_urls'], inplace=True)\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df1_clean fro cleaning task\n",
    "df1_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that there's no missing value\n",
    "df1_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `name` value contains invalid names in lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Access and drop all values in the name column that started with small letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a list that contains all lowercase names\n",
    "lowercase_names = []\n",
    "\n",
    "# create a for loop to gather all lowercase names.\n",
    "for name in df1_clean.name:\n",
    "    if name.islower() and name not in lowercase_names:\n",
    "        lowercase_names.append(name)\n",
    "        \n",
    "print(lowercase_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'lowercase_names' with 'none' in df1_clean\n",
    "df1_clean.name.replace(lowercase_names, 'None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test if lowercase names still exists in df1_clean.name\n",
    "# using a for loop\n",
    "for names in df1_clean.name:\n",
    "    if names.islower():\n",
    "        print(names)\n",
    "        \n",
    "# print should return nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* wrong column name `id` in df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:\n",
    "* change `id` in df3 to `tweet_id` like other dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the rename method to change column head.\n",
    "df3_clean.rename(columns={'id': 'tweet_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `friends_count` has single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "* `friends_count`in df3 is a constant and wont be needed in this analysis, so will be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the drop() function to drop the column\n",
    "df3_clean.drop('friends_count', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 7: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Take out url at the end of some strings in text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use the replace function and strip function on the `text` column to remove the url and whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean['text'] = df1_clean.text.str.replace(r\"https\\S+\", \"\")\n",
    "df1_clean['text'] = df1_clean.text.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# querrying for http to check effect of last line.\n",
    "df1_clean.query(\"text == 'http'\")\n",
    "# this should return and empty column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rating should be in a single column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a new column called `rating` that will contain the expression: `rating_numerator/rating_denominator` on the df1_clean \n",
    "* drop `rating_numerator and denominator` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating rating column with its content\n",
    "df1_clean[\"rating\"] = df1_clean[\"rating_numerator\"]/df1_clean[\"rating_denominator\"]\n",
    "\n",
    "# dropping rating denominator and rating numerator columns\n",
    "df1_clean = df1_clean.drop([\"rating_numerator\", \"rating_denominator\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for new rating column\n",
    "print(df1_clean.rating.sample(20))\n",
    "\n",
    "# check for drooped columns\n",
    "print(df1_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 9: Tidiness Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* doggo, floofer, pupper and puppo columns need to be one column dog_stage with 4 categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* replace empty string rows in `doggo`, `floofer`, `pupper` and `puppo` with None.\n",
    "* combine the four columns to form a `dog_stage column`\n",
    "* use querries to replace mixed dog stage\n",
    "* use the drop() function to drop the 4 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First replace replace empty string rows in \n",
    "# `doggo`, `floofer`, `pupper` and `puppo` with None.\n",
    "#using the replace() function\n",
    "df1_clean.doggo.replace('None', '', inplace=True)\n",
    "df1_clean.floofer.replace('None', '', inplace=True)\n",
    "df1_clean.pupper.replace('None', '', inplace=True)\n",
    "df1_clean.puppo.replace('None', '', inplace=True)\n",
    "\n",
    "# combine the four columns to form a `dog_stage column`\n",
    "df1_clean['dog_stage'] = df1_clean.doggo + df1_clean.floofer + df1_clean.pupper + df1_clean.puppo\n",
    "\n",
    "\n",
    "# use querries to format mixed dog stage \n",
    "df1_clean.loc[df1_clean.dog_stage == \"'doggopupper', 'dog_stage'\"] = 'doggo,pupper'\n",
    "df1_clean.loc[df1_clean.dog_stage == \"'doggopuppo', 'dog_stage'\"] = 'doggo,puppo'\n",
    "df1_clean.loc[df1_clean.dog_stage == \"'doggofloofer', 'dog_stage'\"] = 'doggo,floofer'\n",
    "\n",
    "# use the drop() function to drop the 4 columns\n",
    "df_drop = ['doggo', 'floofer', 'pupper', 'puppo']\n",
    "df1_clean.drop(df_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1_clean.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean.dog_stage.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create extract days from `timestamp column`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use the dt.strftime function to convert `timestamp` to `days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean['timestamp'] = pd.to_datetime(df1_clean['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting day from timestamp\n",
    "df1_clean[\"day\"] = df1_clean[\"timestamp\"].dt.strftime(\"%a\")\n",
    "\n",
    "#dropping timestamp\n",
    "df1_clean.drop('timestamp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_clean[\"day\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue #12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The three DataFrame should be merged as one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "* Use the merge function to merge df1, df2, df3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1_clean, df2_clean, on='tweet_id', how='inner').merge(df3_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "Save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df to csv file\n",
    "df.to_csv(\"..data/twitter_archive_master.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing and Visualizing Data\n",
    "In this section, analyze and visualize your wrangled data. You must produce at least **three (3) insights and one (1) visualization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Relationship between dog `ratings`, `retweets`, `favorites`, `img_number`.\n",
    "2. Explore the data description\n",
    "3. Which day has the most Tweet\n",
    "4. Relationship between Tweet day, and number of retweet and favorites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a copy of df dataset\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights #1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* explore the Relationship between dog `ratings`, `retweets`, `favorites`, `img_number`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a visual plot for this\n",
    "sns.pairplot(df_clean, vars=[\"rating\", \"retweet_count\",\n",
    "                            \"favorite_count\", \"img_num\",\n",
    "                            ], hue=\"day\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation table round off the 3 dp\n",
    "df_clean.corr().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the visual plot and the correlation table above, considering the relationship between `retweeted_count` and `favorite_count`. It can be observed that there's a positive correlation between these two paramters. the correlation table shows a strong `0.913`. this means that the more retweets a tweet gets, the more favorites it will get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight #2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explore the data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `img_num`: here the min img_num for a post that had an image is 1 and the maximium is 4.\n",
    "*`retweet_count` and `favorite_count`: based on the mean, it can be observed that most tweets get a favorite than a retweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* day with the most tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean_d = df_clean.day.value_counts()\n",
    "df_clean_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_d.plot(kind='pie', figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `day column`, monday ranks as the day with the most tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Relationship between tweet day, retweet and favorite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_drf=df_clean.groupby([\"day\"], as_index=False)[\"retweet_count\", \"favorite_count\"].sum()\n",
    "df_clean_drf.sort_values(by=[\"retweet_count\"], ascending = False).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our previous analysis, Monday ranked top with number of tweets. Apparently, though most tweets are made on Mondays, most retweets and favorites are made on Wednesday. so Wednesdays seem to be the most engaging day of WeRateDogs twitter page.\n"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
