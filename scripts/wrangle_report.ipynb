{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA WRANGLING REPORT\n",
    "\n",
    "## Introduction:\n",
    "This project's aim is to reenforce concepts thought in the Udacity-Alx Data Analyst Nano Degree. It aims to train our skillset in Data Wrangling process. For this project, data from WeRateDogs twitter archive was put through the data wrangling procedure. Such procedures as: data gathering, data assessing, data cleaning and data visualization. \n",
    "\n",
    "## Aims and objectives:\n",
    "This project's aim is to wrangle WeRateDog twitter archive. WeRateDog is a Twitter account that rates people's dogs. This wrangling process include:\n",
    "* Data gathering\n",
    "* Data assessing\n",
    "* Data cleaning\n",
    "\n",
    "### Data gathering\n",
    "In this project, data from the archive were gathered to form three datasets. this datasets are:\n",
    "* **`Enhanced Twitter Archive`**: which was programmatically downloaded and provided by udacity for this project. The data was sourced from `twitter archive`. This dataset was then downloaded into my `jupyter notebook` and read as csv file using `pandas read csv() function`. For this gathering, pandas was imported. This dataset wasn't entirely complete,as it lacked data on retweet and favorite numbers. So more data had to be downloaded from the same twitter archive. \n",
    "\n",
    "* **`Tweet image prediction file`**: here, Udacity provided assess to their image prediction file (gotten from their neural network). to get this file, python `requests` and `os` libraries were imported. Using the `get()` function from the `requests` library and the provided data url, the data was programmatically downloaded as saved in a response variable.\n",
    "\n",
    "* **`Tweet_json file`**: This represents the additional data downloaded to make up for the shortcoming of the previous data. In this gathering task, `json`, `timeit`and `tweepy` libraries where imported. from the `tweepy` library, i also imported the `OAuthHandler` and set `default_timer as timer` from the `timeit` library. This data was gathered using the `tweet IDs` in the WeRateDogs `Twitter archive` (found in the first dataset) to query the Twitter archive for each tweet's json data using python's `Tweepy library`. The gathered json data was extracted for paramteres like `retweet_count`, `followers_count`, `favorite_count`. this was done using the python `with open` function, `readline()` and a `for loop`. this extraced file was stored as `tweet_json.txt` file, using . for this gathering to succeed, twitter `access keys` where gotten through creation of a dev account and successful application. \n",
    "\n",
    "### Data Assesing\n",
    "After gathering the three(3) datasets, they were assess for `data quality` and `data tidiness` issues. This was done through `visual` and `programmatic` assessments.\n",
    "* **`Visual Assessment`**: here, the datasets were printed on jupyter and a random sampling was performed to check for `quality` and `tidiness` issues.\n",
    "* **`Programmatic Assessment:`** Here, various pandas functions were called on the various dataset to analyze for `datatype`, `,issing value`, `duplicated values`, and other erroneous data issues. Such functions includes: .`info()`, `.isnull()`, `.head()`, `.tail()`, `.duplicated()`, `value_counts()`.\n",
    "\n",
    "### Data Cleaning\n",
    "Having listed out the issues with the dataset through data assessing, data cleaning is done: `Define`, `Code` and `Test`. Every data issue has to be cleaned through this way.\n",
    "for the cleaning to commence, a copy of the datasets has to be created and used.\n",
    "Some of the cleaning tasks done includes:\n",
    "* datatype correction for `tweet_id`, `timestamp`, `retweeted_status_timestamp` columns in first dataframe.\n",
    "* dropped missing values in numerous columns\n",
    "made the `id` string uniform across all dataframe.\n",
    "* corrected tidiness issues by merging different dog stage column to one and also merging the `rating-numerator` and `Rating_denominator` to one rating column.\n",
    "* Finally, the three cleaned datasets were merged into one.\n",
    "\n",
    "final part was to store the data in a csv file.\n",
    "\n",
    "### Conclusion\n",
    "This project showcased most of the data wrangling tasks needed to handle data. From the actual codes to the debugging that occurs within the process (which is a very important learning point).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e593ac106456af50ce7af38f9671c411b49d6cd90f9b885e167f0f594e09038c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
